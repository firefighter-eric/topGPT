data_path:
model_name_or_path:
model_type:
tokenizer_path:
max_length:

# output
output_dir:
logging_dir:
run_name:
project_name:

# flag
do_eval: true
do_predict: false
do_train: true

# lora
lora: true
load_in_4bit: true
load_in_8bit: false
target_modules: [ q_proj, v_proj, k_proj, o_proj, gate_proj, down_proj, up_proj ]

# hyperparameters
per_device_eval_batch_size: 8
per_device_train_batch_size: 8
learning_rate: 5.0e-05
num_train_epochs: 3.0

warmup_ratio: 0.0
warmup_steps: 0
weight_decay: 0.0

lr_scheduler_type: constant
optim: adamw_torch

gradient_accumulation_steps: 1
gradient_checkpointing: true

# backend
torch_compile: true
#torch_compile_backend: None
#torch_compile_mode: None
deepspeed: None

# precision
bf16: true
bf16_full_eval: true
fp16: false
fp16_backend: auto
fp16_full_eval: false
fp16_opt_level: O1
tf32: true

# data
seed: 42
data_seed: None
dataloader_drop_last: false
dataloader_num_workers: 0
dataloader_pin_memory: true
remove_unused_columns: true

# eval
eval_delay: 0
eval_steps: 100
evaluation_strategy: steps
prediction_loss_only: True

#greater_is_better: None
#metric_for_best_model: None

# log
log_level: passive
log_level_replica: warning
log_on_each_node: true
logging_first_step: true
logging_nan_inf_filter: true
logging_steps: 10
logging_strategy: steps
report_to:
  - wandb
disable_tqdm: false

# save
save_safetensors: false
save_steps: 100
save_strategy: steps
save_total_limit: 2
load_best_model_at_end: true

#resume_from_checkpoint: None


