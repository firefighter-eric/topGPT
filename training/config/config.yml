data_path:
model_name_or_path:
model_type:
tokenizer_path:
max_length:

# output
output_dir:
logging_dir:
run_name:
project_name:

# flag
do_eval: true
do_predict: false
do_train: true

# hyperparameters
per_device_eval_batch_size: 8
per_device_train_batch_size: 8
learning_rate: 5.0e-05
num_train_epochs: 3.0

warmup_ratio: 0.0
warmup_steps: 0
weight_decay: 0.0

lr_scheduler_type: constant
optim: adamw_torch

gradient_accumulation_steps: 1
gradient_checkpointing: true

torch_compile: true
#torch_compile_backend: None
#torch_compile_mode: None

# precision
bf16: true
bf16_full_eval: true
fp16: false
fp16_backend: auto
fp16_full_eval: false
fp16_opt_level: O1
tf32: true

# data
data_seed: None
dataloader_drop_last: false
dataloader_num_workers: 0
dataloader_pin_memory: true

deepspeed: None
disable_tqdm: false

# eval
eval_delay: 0
eval_steps: 100
evaluation_strategy: steps
prediction_loss_only: True

#greater_is_better: None
#metric_for_best_model: None

label_smoothing_factor: 0.0


# log
log_level: passive
log_level_replica: warning
log_on_each_node: true
logging_first_step: true
logging_nan_inf_filter: true
logging_steps: 10
logging_strategy: steps
report_to:
  - wandb

remove_unused_columns: true

#resume_from_checkpoint: None

# save
save_safetensors: false
save_steps: 100
save_strategy: steps
save_total_limit: 2
load_best_model_at_end: true

seed: 42
skip_memory_metrics: true




